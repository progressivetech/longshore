#!/bin/bash

long_usage_message="usage: $(basename $0) site-backup [<site>] [<name>]
Create a backup given site or all sites that this sibling manages.

Arguments
  <site> - Optionally pass the site if you only want that site backed up.
  <name> - Optionally pass the base name you would like the backup file
  to use. Defaults to 'daily' to produce <site>.daily.tar.gz."

long_check_args "$2" "$#" 0

# Our backup strategy is:
# 1. Make daily backups that overwrite the previous day's backup
# 2. Make first of the month backups that persist for 12 months on the server
#    on which the database lives.
# 3. Copy daily backups and the last 3 months backups to every server to
#    facilitate restoring in case a server is lost (this step is handled by
#    the sibling-backup script).

# This scripts is designed to be run by a daily cron job.  It makes a backup
# called site.daily.tar.gz. In addition, One the first of every month it makes
# a timestamped backup designed to stay around for 1 year.  After a year, we
# should have 12 backups of each database.  Backups older than one year are
# deleted from all directories. Backups older than 3 months are deleted from
# directories not matching $hostname.

hostname=$(hostname)
backup_dir="${LONG_BACKUPS}"

if [ -z "$backup_dir" ]; then
  long_die "LONG_BACKUPS is not defined.\n"
fi

my_backup_dir="${backup_dir}/${hostname}"

[ ! -d "${my_backup_dir}" ] && mkdir -p "${my_backup_dir}"

# delete all backups older than one year from this server's backup directory.
for file in $(find "${backup_dir}/" -mtime +365); do
  long_log "Deleting %s because it is more than a year old." info "$file"
  rm "$file"
done

# delete all backups older than 3 months from other servers backups to save
for dir in $(ls "${backup_dir}/"); do
  if [ "$hostname" != "$dir" ]; then
    for file in $(find "${backup_dir}/${dir}" -type f -mtime +90 2>/dev/null); do
      long_log "Deleting %s because it is more than 3 months old." info "$file"
      rm "$file"
    done
  fi
done

# Run for all local sites
if [ -n "$2" ]; then
  sites="$2"
else
  long_set_sites "$hostname"
  sites="$LONG_SITES"
fi

if [ -n "$3" ]; then
  base_name="$3"
  base_test=$(echo "$base_name" | tr -d "[:punct:]")
  if [ "$base_name" != "$base_test" ]; then
    long_die "Please don't use punctuation in base name."
  fi
else
  base_name=daily
fi

day=$(date +%d)
date=$(date +%Y%m%d)

for site in $sites; do
  if ! long_check_site_exists "$site"; then
    long_log "Site doesn't exist (%s)." error "$site"
    continue
  fi
  long_log "Starting backup of %s." info "$site"

  # If the site is not running, don't backup (otherwise we might end up with an
  # empty database, if the database container is not running).  This also
  # ensures that we aren't trying to back up a site from a different sibling.
  # Backups are designed to be run on each sibling and for each sibling
  # independently of the other siblings.
  if ! long_container_running "$site"; then
    long_log "Not backing up %s because it is not running." info "$site"
    continue
  fi

  # Clear caches to reduce database backup size.
  long_exec_drush_cmd "$site" "cc all"

  long_log "Dumping the database."

  # Note: If the database dump fails, the file won't be moved into place (see
  # long_dump_database), so we don't run the risk of overwriting a good
  # database dump with a bad one.  For that reason, we continue with the
  # backup, without checking the exit code of the db dump.
  long_dump_database "$site"

  # Create temp file to store backup. We won't move it into place until
  # we know it was successful to avoid overwriting a good backup with a
  # bad backup.
  target=$(mktemp -t XXXXXXXXX.tar.gz)
  source_dir="${LONG_SRV}/services/sites/"
  long_log "Backing up %s to %s." info "$site" "$target"
  out=$(tar -czf "$target" --directory "$source_dir" \
    --ignore-failed-read \
    --exclude "${site}/drupal/files/civicrm/templates_c/*" \
    --exclude "${site}/drupal/files/css/*" \
    --exclude "${site}/drupal/files/js/*" \
    --exclude "${site}/drupal/private/temp/*" \
    "$site" \
    2>&1)
  if [ "$?" -eq "0" -a -z "$out" ]; then
    # --ignore-failed-read will allow us to continue even if a file disapears
    # during backup ("file changed as we read it") or if a file is un-readable
    # due to permission problems (which should be fixed, but we don't want it
    # to stop the rest of the backup from happening). Because we are using
    # --ignore-failed-read, we will continue with a zero-exit code, but the
    # output should have an error. 
    # 
    # If the backup is fully successful we should have a zero exit code and no
    # output.
    long_log "Back up of %s successful." info "$site"
  fi
  if [ "$?" -eq "0" ]; then
    # Either we were successful, or we were successful but had a read error.
    if [ -n "$out" ]; then
      # This means we have a read error, but otherwise the backup was
      # successful.
      long_log "Sucessful backup, but read error: %s (%s)" error "$site" "$out"
      long_debug "$out" "Successful backup, but read error: $site"
    fi
    # If backup was successful, regardless of possible read errors... move into
    # the backups directory, over writing the previous nights backup. We run a small
    # risk of overwriting a good backup, but I expect that all read errors will be
    # about a handful of temporary files. We don't want to abort the entire backup
    # because a few files were un-readable.
    daily_path="$my_backup_dir/${site}.${base_name}.tar.gz"
    long_log "Moving %s to %s." info "$target" "$daily_path"
    mv "$target" "$daily_path"
    if [ "$day" == "01" ]; then
      monthly_path="$my_backup_dir/${site}.$date.tar.gz"
      long_log "Creating monthly backup of %s to %s" "$site" "$monthly_path"
      cp "$daily_path" "$monthly_path"
    fi
  else
    # If we have a more serious failure, don't overwrite a previous good
    # backup with a bad one.
    [ -f "$target" ] && rm "$target"
    long_log "tar command failed for %s (%s)" error "$site" "$out"
    long_debug "$out" "backup failure: $site"
  fi
done
